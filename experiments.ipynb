{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2431805,"sourceType":"datasetVersion","datasetId":8782}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        # print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:57:33.949352Z","iopub.execute_input":"2025-07-09T12:57:33.949629Z","iopub.status.idle":"2025-07-09T12:57:44.703837Z","shell.execute_reply.started":"2025-07-09T12:57:33.949607Z","shell.execute_reply":"2025-07-09T12:57:44.703275Z"},"_kg_hide-output":false,"scrolled":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install torchvision torch faiss-cpu opencv-python-headless timm torchmetrics pytorch-metric-learning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:57:44.705026Z","iopub.execute_input":"2025-07-09T12:57:44.705435Z","iopub.status.idle":"2025-07-09T12:58:55.832720Z","shell.execute_reply.started":"2025-07-09T12:57:44.705410Z","shell.execute_reply":"2025-07-09T12:58:55.831794Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.3)\nCollecting pytorch-metric-learning\n  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (4.67.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-metric-learning, faiss-cpu\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-metric-learning-2.8.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport torch\nfrom types import SimpleNamespace\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.use_deterministic_algorithms(True, warn_only=True)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark     = False\nset_seed(42)\n\n\ninput_dirs = glob.glob('/kaggle/input/*flowers*')\nif input_dirs:\n    dataset_root = input_dirs[0]\nelse:\n    dataset_root = '../input/flowers-recognition/flowers'\n\ncfg = SimpleNamespace(\n    seed=42,\n    verbose=True,\n    dataset=SimpleNamespace(\n        root=dataset_root,\n        image_size=224,\n        num_workers=0\n    ),\n    splits=SimpleNamespace(\n        training_ratio=0.8\n    ),\n    training=SimpleNamespace(\n        batch_size=32,\n        epochs=15,\n        lr_backbone=1e-4,\n        lr_head=1e-3\n    ),\n    index=SimpleNamespace(\n        type='hnsw',\n        metric='cosine',\n        hnsw=SimpleNamespace(\n            M=32,\n            ef_construction=200,\n            ef_search=50\n        )\n    ),\n    models=[\n        'resnet50',\n        'efficientnet_b0',\n        'clip_zeroshot',\n        'clip_finetune',\n        'metric_learning',\n        'dinov2',\n        ''\n    ],\n    evaluation=SimpleNamespace(\n        k=5\n    ),\n    api=SimpleNamespace(\n        host='0.0.0.0',\n        port=8000\n    )\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:58:55.833885Z","iopub.execute_input":"2025-07-09T12:58:55.834172Z","iopub.status.idle":"2025-07-09T12:58:58.763559Z","shell.execute_reply.started":"2025-07-09T12:58:55.834147Z","shell.execute_reply":"2025-07-09T12:58:58.762795Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from pathlib import Path\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport random, os, torch, numpy as np\n\nclass FlowersDataset(Dataset):\n\n    def __init__(self, root, transform=None):\n        self.samples, self.labels = [], []\n        classes = sorted(os.listdir(root))\n        self.class_to_idx = {c:i for i,c in enumerate(classes)}\n        for cls in classes:\n            for p in Path(root, cls).glob('*'):\n                self.samples.append(str(p))\n                self.labels.append(self.class_to_idx[cls])\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.samples[idx]).convert('RGB')\n        if self.transform: img = self.transform(img)\n        return img, self.labels[idx], self.samples[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:58:58.765084Z","iopub.execute_input":"2025-07-09T12:58:58.765453Z","iopub.status.idle":"2025-07-09T12:59:02.483362Z","shell.execute_reply.started":"2025-07-09T12:58:58.765434Z","shell.execute_reply":"2025-07-09T12:59:02.482816Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"MEAN = (0.48145466, 0.4578275, 0.40821073)\nSTD  = (0.26862954, 0.26130258, 0.27577711)\n\nclip_val_tf = transforms.Compose([\n    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.484003Z","iopub.execute_input":"2025-07-09T12:59:02.484529Z","iopub.status.idle":"2025-07-09T12:59:02.489254Z","shell.execute_reply.started":"2025-07-09T12:59:02.484510Z","shell.execute_reply":"2025-07-09T12:59:02.488606Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def set_seed(seed: int = 42):\n    # 1 Python\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n    # 2 NumPy\n    np.random.seed(seed)\n\n    # 3 PyTorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.use_deterministic_algorithms(True, warn_only=True)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark     = False\n\n    # 4 FAISS (HNSW), sklearn, etc.\n    try:\n        import faiss\n        faiss.rand.seed(seed)\n    except ImportError:\n        pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.490266Z","iopub.execute_input":"2025-07-09T12:59:02.490586Z","iopub.status.idle":"2025-07-09T12:59:02.505098Z","shell.execute_reply.started":"2025-07-09T12:59:02.490539Z","shell.execute_reply":"2025-07-09T12:59:02.504526Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"full_ds = FlowersDataset(\"/kaggle/input/flowers-recognition/flowers\", None)\nidxs = list(range(len(full_ds)))\nrandom.Random(42).shuffle(idxs)\n\nfrom collections import defaultdict\ncls_bins = defaultdict(list)\nfor i in idxs:\n    cls_bins[full_ds.labels[i]].append(i)\n\ntrain_idx, test_idx = [], []\nfor lst in cls_bins.values():\n    cut = int(len(lst) * 0.8)\n    train_idx += lst[:cut]\n    test_idx  += lst[cut:]\n\ntrain_set = torch.utils.data.Subset(full_ds, train_idx)\ntest_set  = torch.utils.data.Subset(full_ds, test_idx)\ntrain_set.dataset.transform = clip_val_tf\ntest_set.dataset.transform  = clip_val_tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.505755Z","iopub.execute_input":"2025-07-09T12:59:02.505931Z","iopub.status.idle":"2025-07-09T12:59:02.544726Z","shell.execute_reply.started":"2025-07-09T12:59:02.505916Z","shell.execute_reply":"2025-07-09T12:59:02.543967Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import abc\nclass FeatureExtractor(abc.ABC):\n    dim: int\n    @abc.abstractmethod\n    def fit(self, loader): pass\n    @abc.abstractmethod\n    def encode(self, images): pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.545471Z","iopub.execute_input":"2025-07-09T12:59:02.545716Z","iopub.status.idle":"2025-07-09T12:59:02.550044Z","shell.execute_reply.started":"2025-07-09T12:59:02.545691Z","shell.execute_reply":"2025-07-09T12:59:02.549294Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from copy import deepcopy\n\nclass FineTuneMixin:\n    def _make_val_split(self, train_subset, val_ratio=1-cfg.splits.training_ratio):\n        idx = train_subset.indices if hasattr(train_subset, \"indices\") else list(range(len(train_subset)))\n        split = int(len(idx) * (1 - val_ratio))\n        return (\n            torch.utils.data.Subset(train_subset.dataset, idx[:split]),\n            torch.utils.data.Subset(train_subset.dataset, idx[split:])\n        )\n\n    def _train_one_epoch(self, loader, criterion, optimizer, scheduler):\n        self.backbone.train()\n        running_loss, correct, total = 0.0, 0, 0\n        for x, y, _ in tqdm(loader, disable=not cfg.verbose, leave=False):\n            x, y = x.cuda(), y.cuda()\n            optimizer.zero_grad()\n            out = self.backbone(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            if scheduler: scheduler.step()\n\n            running_loss += loss.item() * x.size(0)\n            preds = out.argmax(dim=1)\n            correct += (preds == y).sum().item()\n            total += x.size(0)\n        return running_loss / total, correct / total\n\n    @torch.no_grad()\n    def _validate(self, loader, criterion):\n        self.backbone.eval()\n        vl, vc, vt = 0.0, 0, 0\n        for x, y, _ in loader:\n            x, y = x.cuda(), y.cuda()\n            out = self.backbone(x)\n            loss = criterion(out, y)\n            vl += loss.item() * x.size(0)\n            vc += (out.argmax(1) == y).sum().item()\n            vt += x.size(0)\n        return vl / vt, vc / vt\n\n    def _fine_tune(self, train_loader, *, max_epochs=cfg.training.epochs, patience=3,\n                   lr_head=cfg.training.lr_head, lr_base=cfg.training.lr_backbone, weight_decay=1e-4):\n        train_ds, val_ds = self._make_val_split(train_loader.dataset, 0.1)\n        train_dl = torch.utils.data.DataLoader(\n            train_ds, batch_size=train_loader.batch_size, shuffle=True,\n            num_workers=cfg.dataset.num_workers, drop_last=True)\n        val_dl = torch.utils.data.DataLoader(\n            val_ds, batch_size=train_loader.batch_size, shuffle=False,\n            num_workers=cfg.dataset.num_workers)\n\n        optim = torch.optim.AdamW(\n            [\n                {\"params\": self.head_params, \"lr\": lr_head, \"weight_decay\": weight_decay},\n                {\"params\": self.base_params, \"lr\": lr_base, \"weight_decay\": weight_decay}\n            ]\n        )\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=max_epochs*len(train_dl))\n        criterion = getattr(self, \"loss_fn\", nn.CrossEntropyLoss())\n        best_wts, best_acc, wait = deepcopy(self.backbone.state_dict()), 0.0, 0\n\n        for epoch in range(1, max_epochs + 1):\n            tl, ta = self._train_one_epoch(train_dl, criterion, optim, sched)\n            vl, va = self._validate(val_dl, criterion)\n            print(f\"Epoch {epoch:02d}: train loss={tl:.4f} acc={ta:.3f} | \"\n                  f\"val loss={vl:.4f} acc={va:.3f}\")\n            if va > best_acc + 1e-4:\n                best_acc, best_wts, wait = va, deepcopy(self.backbone.state_dict()), 0\n            else:\n                wait += 1\n                if wait >= patience:\n                    print(\"Early stopping triggered\")\n                    break\n\n        self.backbone.load_state_dict(best_wts)\n        print(f\"Best val acc={best_acc:.3f} (epoch {epoch-wait})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.550888Z","iopub.execute_input":"2025-07-09T12:59:02.551134Z","iopub.status.idle":"2025-07-09T12:59:02.565776Z","shell.execute_reply.started":"2025-07-09T12:59:02.551113Z","shell.execute_reply":"2025-07-09T12:59:02.565186Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ResNetExtractor(FineTuneMixin, FeatureExtractor):\n    def __init__(self, num_classes=5):\n        self.backbone = timm.create_model('resnet50', pretrained=True, drop_rate=0.2)\n        self.backbone.reset_classifier(num_classes)\n        self.dim = self.backbone.num_features\n\n        classifier = self.backbone.get_classifier()\n        self.head_params = list(classifier.parameters())\n        self.base_params = [p for p in self.backbone.parameters() if id(p) not in set(map(id, self.head_params))]\n\n    def fit(self, train_loader):\n        print(\"Fine tune ResNet50\")\n        self.backbone.cuda()\n        self._fine_tune(train_loader,\n                        max_epochs=cfg.training.epochs,\n                        patience=3,\n                        lr_head=cfg.training.lr_head,\n                        lr_base=cfg.training.lr_backbone)\n\n    @staticmethod\n    def _pool(feats):\n        \"\"\"\n        B*C*H*W  ->  B*C или оставляет если уже\n        \"\"\"\n        if feats.ndim == 4:\n            feats = feats.mean(dim=(-1, -2))\n        elif feats.ndim == 3:\n            feats = feats.squeeze(-1)\n        return torch.nn.functional.normalize(feats, dim=-1)\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            feats = self.backbone.forward_features(images.cuda())\n            #print(feats.shape)\n            feats = self._pool(feats)\n            return feats.cpu().numpy().astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.568183Z","iopub.execute_input":"2025-07-09T12:59:02.568436Z","iopub.status.idle":"2025-07-09T12:59:02.580982Z","shell.execute_reply.started":"2025-07-09T12:59:02.568420Z","shell.execute_reply":"2025-07-09T12:59:02.580236Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class EfficientNetExtractor(FineTuneMixin, FeatureExtractor):\n    def __init__(self, num_classes=5):\n        self.backbone = timm.create_model(\n            'efficientnet_b0',\n            pretrained=True,\n            drop_rate=0.2\n        )\n        self.backbone.reset_classifier(num_classes)\n        self.dim = self.backbone.num_features\n        classifier = self.backbone.get_classifier()\n        self.head_params = list(classifier.parameters())\n        self.base_params = [p for p in self.backbone.parameters()\n                            if id(p) not in set(map(id, self.head_params))]\n\n    def fit(self, train_loader):\n        print(\"Fine tune EfficientNet-B0\")\n        self.backbone.cuda()\n        self._fine_tune(train_loader,\n                        max_epochs=cfg.training.epochs,\n                        patience=3,\n                        lr_head=cfg.training.lr_head,\n                        lr_base=cfg.training.lr_backbone)\n    @staticmethod\n    def _pool(feats):\n        \"\"\"\n        B*C*H*W  ->  B*C или оставляет если уже\n        \"\"\"\n        if feats.ndim == 4:\n            feats = feats.mean(dim=(-1, -2))\n        elif feats.ndim == 3:\n            feats = feats.squeeze(-1)\n        return torch.nn.functional.normalize(feats, dim=-1)\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            feats = self.backbone.forward_features(images.cuda())\n            #print(feats.shape)\n            feats = self._pool(feats)\n            return feats.cpu().numpy().astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.581972Z","iopub.execute_input":"2025-07-09T12:59:02.582198Z","iopub.status.idle":"2025-07-09T12:59:02.597515Z","shell.execute_reply.started":"2025-07-09T12:59:02.582182Z","shell.execute_reply":"2025-07-09T12:59:02.596835Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip install transformers accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:02.598319Z","iopub.execute_input":"2025-07-09T12:59:02.598728Z","iopub.status.idle":"2025-07-09T12:59:05.976207Z","shell.execute_reply.started":"2025-07-09T12:59:02.598706Z","shell.execute_reply":"2025-07-09T12:59:05.975116Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nclass CLIPHFExtractor(FeatureExtractor):\n    \"\"\"\n    CLIP ViT-B/32 по умолчанию без fine tune\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        self.device = device\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.model     = CLIPModel.from_pretrained(model_name).to(device)\n        self.model.eval()\n        self.dim = self.model.config.projection_dim\n        self.to_pil = transforms.ToPILImage()\n\n    def fit(self, *_): pass\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            pil = [self.to_pil(img.cpu()) for img in images]\n        else:\n            pil = [Image.open(p).convert(\"RGB\") for p in images]\n        inputs = self.processor(images=pil, return_tensors=\"pt\", padding=True).to(self.device)\n        feats = self.model.get_image_features(**inputs)\n        feats = F.normalize(feats, p=2, dim=-1)\n        return feats.cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:05.977465Z","iopub.execute_input":"2025-07-09T12:59:05.978348Z","iopub.status.idle":"2025-07-09T12:59:23.297118Z","shell.execute_reply.started":"2025-07-09T12:59:05.978321Z","shell.execute_reply":"2025-07-09T12:59:23.296581Z"}},"outputs":[{"name":"stderr","text":"2025-07-09 12:59:09.369966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752065949.541702      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752065949.594376      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from pytorch_metric_learning import losses, miners\nimport torch.nn as nn\nimport inspect\nclass MetricExtractor(nn.Module):\n    \"\"\"\n    ResNet-18 backbone + Linear(256) + TripletLoss\n    \"\"\"\n    def __init__(self, embed_dim: int = 256):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"resnet18\", pretrained=True, num_classes=0, drop_rate=0.2\n        )\n        self.embed = nn.Linear(self.backbone.num_features, embed_dim)\n        self.dim   = embed_dim\n\n        self.loss_fn = losses.TripletMarginLoss(margin=0.2)\n        self.miner   = miners.TripletMarginMiner(\n            margin=0.2, type_of_triplets=\"semihard\"\n        )\n\n    def fit(\n        self,\n        dl: DataLoader,\n        *,\n        epochs: int = cfg.training.epochs,\n        patience: int = 4,\n        lr_head: float = cfg.training.lr_head,\n        lr_base: float = cfg.training.lr_backbone,\n        weight_decay: float = 1e-4,\n    ):\n        idx = dl.dataset.indices if hasattr(dl.dataset, \"indices\") else range(len(dl.dataset))\n        n_val = int(0.1 * len(idx))\n        train_idx, val_idx = idx[:-n_val], idx[-n_val:]\n\n        base_ds = dl.dataset.dataset if hasattr(dl.dataset, \"dataset\") else dl.dataset\n        train_ds = Subset(base_ds, train_idx)\n        val_ds   = Subset(base_ds, val_idx)\n\n        train_dl = DataLoader(\n            train_ds, batch_size=dl.batch_size, shuffle=True,\n            num_workers=cfg.dataset.num_workers, drop_last=True\n        )\n        val_dl = DataLoader(\n            val_ds, batch_size=dl.batch_size, shuffle=False,\n            num_workers=cfg.dataset.num_workers\n        )\n\n        opt = torch.optim.AdamW(\n            [\n                {\"params\": self.embed.parameters(), \"lr\": lr_head, \"weight_decay\": weight_decay},\n                {\"params\": self.backbone.parameters(), \"lr\": lr_base, \"weight_decay\": weight_decay},\n            ]\n        )\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt, T_max=epochs * len(train_dl)\n        )\n\n        self.backbone.cuda(); self.embed.cuda()\n\n        best_knn, wait = 0.0, 0\n        best_state     = {k: v.clone() for k, v in self.state_dict().items()}\n\n        for ep in range(1, epochs + 1):\n\n            self.train()\n            run_loss, seen = 0.0, 0\n            for x, y, _ in tqdm(train_dl, leave=False, desc=f\"E{ep:02d} train\"):\n                x, y = x.cuda(), y.cuda()\n                opt.zero_grad()\n                emb = F.normalize(self.embed(self.backbone(x)), dim=-1)\n                hard = self.miner(emb, y)\n                loss = self.loss_fn(emb, y, hard)\n                loss.backward(); opt.step(); sched.step()\n                run_loss += loss.item() * x.size(0)\n                seen     += x.size(0)\n            train_loss = run_loss / seen\n            knn_acc = self._val_knn_acc(train_dl, val_dl, k=5)\n\n            print(f\"Epoch {ep:02d}: loss={train_loss:.4f} | val kNN@1={knn_acc:.3f}\")\n\n            if knn_acc > best_knn + 1e-4:\n                best_knn, best_state, wait = knn_acc, \\\n                    {k: v.clone() for k, v in self.state_dict().items()}, 0\n            else:\n                wait += 1\n                if wait >= patience:\n                    print(\"Early stopping.\")\n                    break\n\n        self.load_state_dict(best_state)\n        print(f\"Best val kNN@1 = {best_knn:.3f}\")\n\n    @torch.no_grad()\n    def _val_knn_acc(self, train_dl: DataLoader, val_dl: DataLoader, *, k: int = 5) -> float:\n        \"\"\"\n        Строит Flat-индекс из train-эмбеддингов и меряет top-1 точность на val\n        \"\"\"\n        tr_vecs, tr_lbls = [], []\n        for x, y, _ in train_dl:\n            z = F.normalize(self.embed(self.backbone(x.cuda())), dim=-1)\n            tr_vecs.append(z.cpu()); tr_lbls.append(y)\n        tr_vecs = torch.cat(tr_vecs).numpy().astype(\"float32\")\n        tr_lbls = torch.cat(tr_lbls).numpy()\n\n        faiss.normalize_L2(tr_vecs)\n        index = faiss.IndexFlatIP(tr_vecs.shape[1])\n        index.add(tr_vecs)\n\n        correct, total = 0, 0\n        for x, y, _ in val_dl:\n            q = F.normalize(self.embed(self.backbone(x.cuda())), dim=-1).cpu().numpy()\n            faiss.normalize_L2(q)\n            _, I = index.search(q, k)                # (B,k)\n            pred = tr_lbls[I[:, 0]]                  # ближайший сосед\n            correct += np.sum(pred == y.numpy())\n            total   += y.size(0)\n        return correct / total\n\n    @torch.no_grad()\n    def encode(self, images):\n        \"\"\"Возвращает numpy (B,D)\"\"\"\n        if isinstance(images, torch.Tensor):\n            x = images.cuda()\n        else:\n            x = torch.stack([clip_val_tf(Image.open(p).convert(\"RGB\")) for p in images]).cuda()\n\n        z = F.normalize(self.embed(self.backbone(x)), dim=-1)\n        return z.cpu().numpy().astype(\"float32\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:23.297975Z","iopub.execute_input":"2025-07-09T12:59:23.298581Z","iopub.status.idle":"2025-07-09T12:59:23.335753Z","shell.execute_reply.started":"2025-07-09T12:59:23.298529Z","shell.execute_reply":"2025-07-09T12:59:23.334986Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class _ClipImageEncoder(nn.Module):\n    def __init__(self, clip_model):\n        super().__init__()\n        self.clip = clip_model\n\n    def forward(self, x):                 # x: B*3*H*W\n        return self.clip.get_image_features(pixel_values=x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:23.337348Z","iopub.execute_input":"2025-07-09T12:59:23.337594Z","iopub.status.idle":"2025-07-09T12:59:23.734177Z","shell.execute_reply.started":"2025-07-09T12:59:23.337574Z","shell.execute_reply":"2025-07-09T12:59:23.733452Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class _Wrapper:\n    def __init__(self, loader, fn_map):\n        self.loader = loader\n        self.fn_map = fn_map\n        self.batch_size = loader.batch_size\n        self.dataset = loader.dataset\n    def __iter__(self):\n        return self.fn_map(self.loader)\n    def __len__(self):\n        return len(self.loader)\n\nclass CLIPFineTuneExtractor(FineTuneMixin, FeatureExtractor):\n    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.clip      = CLIPModel.from_pretrained(model_name).to(self.device)\n        self.dim       = self.clip.config.projection_dim\n        self.head_params = list(self.clip.visual_projection.parameters())\n        self.base_params = list(self.clip.vision_model.parameters())\n\n        for p in self.clip.text_model.parameters():\n            p.requires_grad = False\n\n        self.backbone = _ClipImageEncoder(self.clip)\n        self.t2pil    = transforms.ToPILImage()\n\n    def fit(self, train_loader):\n        print(\"Fine tune CLIP\")\n\n        def _tensor_loader(dl):\n            for x, y, pths in dl:\n                pil = [self.t2pil(img.cpu()) for img in x]\n                px  = self.processor(images=pil, return_tensors=\"pt\", padding=True)[\"pixel_values\"]\n                yield px, y, pths\n\n        wrapped = _Wrapper(train_loader, _tensor_loader)\n        self.clip.train()\n        self._fine_tune(\n            wrapped,\n            max_epochs=cfg.training.epochs,\n            patience=2,\n            lr_head=cfg.training.lr_head,\n            lr_base=cfg.training.lr_backbone\n        )\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            pil = [self.t2pil(img.cpu()) for img in images]\n        else:\n            pil = [Image.open(p).convert(\"RGB\") for p in images]\n\n        px = self.processor(images=pil, return_tensors=\"pt\", padding=True)[\"pixel_values\"].to(self.device)\n        feats = self.clip.get_image_features(pixel_values=px)\n        return F.normalize(feats, p=2, dim=-1).cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:23.735067Z","iopub.execute_input":"2025-07-09T12:59:23.735326Z","iopub.status.idle":"2025-07-09T12:59:23.751037Z","shell.execute_reply.started":"2025-07-09T12:59:23.735304Z","shell.execute_reply":"2025-07-09T12:59:23.750452Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class DINOv2Extractor(FeatureExtractor):\n    def __init__(self, model_name: str = \"vit_base_patch14_dinov2\"):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.IMG = 518\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=True,\n            num_classes=0,\n            img_size=self.IMG,          # задаём правильный размер тк дино на 518\n        ).to(self.device).eval()\n\n        self.dim = self.backbone.num_features\n\n        self.tf = transforms.Compose([\n            transforms.Resize(self.IMG, interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.CenterCrop(self.IMG),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                                 std=(0.229, 0.224, 0.225)),\n        ])\n\n    def fit(self, *_): \n        pass\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            x = images.to(self.device)\n            if x.shape[-1] != self.IMG:\n                x = F.interpolate(x, size=self.IMG, mode=\"bicubic\", align_corners=False)\n        else:\n            x = torch.stack([self.tf(Image.open(p).convert(\"RGB\")) for p in images]).to(self.device)\n\n        feats = self.backbone(x)            # B * 768\n        feats = F.normalize(feats, dim=-1)\n        return feats.cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T13:05:28.869936Z","iopub.execute_input":"2025-07-09T13:05:28.870594Z","iopub.status.idle":"2025-07-09T13:05:28.877586Z","shell.execute_reply.started":"2025-07-09T13:05:28.870570Z","shell.execute_reply":"2025-07-09T13:05:28.876923Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import faiss\n\n\ndef build_index(vecs: np.ndarray, metric: str = \"cosine\") -> faiss.Index:\n    \"\"\"\n    vecs – (N,D) float32,\n    metric: 'cosine' | 'l2'\n    \"\"\"\n    vecs = np.ascontiguousarray(vecs, dtype=\"float32\")\n\n    if metric == \"cosine\":\n        faiss.normalize_L2(vecs)\n        index = faiss.IndexFlatIP(vecs.shape[1])\n    elif metric == \"l2\":\n        index = faiss.IndexFlatL2(vecs.shape[1])\n    else:\n        raise ValueError(\"метрика должна быть 'cosine' or 'l2'\")\n\n    index.add(vecs)\n    return index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:23.766412Z","iopub.execute_input":"2025-07-09T12:59:23.766691Z","iopub.status.idle":"2025-07-09T12:59:23.808857Z","shell.execute_reply.started":"2025-07-09T12:59:23.766663Z","shell.execute_reply":"2025-07-09T12:59:23.808098Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom sklearn.metrics import average_precision_score\n\ndef evaluate(extractor, train_loader, test_loader):\n    \"\"\"\n    → Precision@5, mAP, faiss-index, rel_paths\n    \"\"\"\n    extractor.fit(train_loader)\n\n    # ---------- кодируем базу без shuffle ----------\n    enc_loader = DataLoader(\n        train_loader.dataset,\n        batch_size=train_loader.batch_size,\n        shuffle=False,\n        num_workers=cfg.dataset.num_workers,\n    )\n\n    db_vecs, db_labels, rel_paths = [], [], []\n    for x, y, paths in enc_loader:\n        db_vecs.append(extractor.encode(x))\n        db_labels.extend(y.numpy())\n        rel_paths.extend([os.path.relpath(p, cfg.dataset.root) for p in paths])\n\n    db_vecs   = np.vstack(db_vecs).astype(\"float32\")\n    db_labels = np.asarray(db_labels)\n\n    if cfg.index.metric == \"cosine\":\n        faiss.normalize_L2(db_vecs)\n        index = faiss.IndexFlatIP(db_vecs.shape[1])\n    else:\n        index = faiss.IndexFlatL2(db_vecs.shape[1])\n    index.add(db_vecs)\n\n    # ---------- Precision@5 и mAP ----------\n    k = cfg.evaluation.k\n    prec_sum, ap_list = 0.0, []\n    for x, y, _ in test_loader:\n        q = extractor.encode(x)\n        if cfg.index.metric == \"cosine\":\n            faiss.normalize_L2(q)\n        D, I = index.search(q, db_vecs.shape[0])        # полный ранжированный список\n        for lbl, d_row, i_row in zip(y.numpy(), D, I):\n            prec_sum += np.sum(db_labels[i_row[:k]] == lbl) / k\n            rel = (db_labels[i_row] == lbl).astype(int)\n            score = d_row if cfg.index.metric != \"l2\" else -d_row\n            ap_list.append(average_precision_score(rel, score))\n\n    precision = prec_sum / len(test_loader.dataset)\n    mAP = float(np.mean(ap_list))\n    return precision, mAP, index, rel_paths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:59:23.809743Z","iopub.execute_input":"2025-07-09T12:59:23.809951Z","iopub.status.idle":"2025-07-09T12:59:23.818042Z","shell.execute_reply.started":"2025-07-09T12:59:23.809934Z","shell.execute_reply":"2025-07-09T12:59:23.817296Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from collections import Counter\nfrom pathlib import Path\nfrom datetime import datetime\nimport torch.nn.functional as F\nfrom torch.utils.data import Subset\n\nimport timm\n\nextractors = {\n    'resnet50'      : ResNetExtractor(),\n    'efficientnet_b0': EfficientNetExtractor(),\n    'clip_zeroshot' : CLIPHFExtractor(),\n    'metric_learning': MetricExtractor(),\n    'clip_finetune' : CLIPFineTuneExtractor(),\n    'dinov2'        : DINOv2Extractor()\n}\n\ntrain_loader = DataLoader(train_set, batch_size=cfg.training.batch_size, \n                          shuffle=False, num_workers=cfg.dataset.num_workers, drop_last=True)\ntest_loader  = DataLoader(test_set,  batch_size=cfg.training.batch_size, \n                          shuffle=False, num_workers=cfg.dataset.num_workers)\nsave_dir = Path(\"checkpoints\")\nsave_dir.mkdir(exist_ok=True, parents=True)\nscores = {}\nfor name, extractor in extractors.items():\n    print(f\"\\n=== {name.upper()} ===\")\n    p, mAP, idx, rel_paths = evaluate(extractor, train_loader, test_loader)\n    scores[name] = (p, mAP)\n    print(f\"Precision@{cfg.evaluation.k}: {p:.3f}\")\n    print(f\"mAP@{cfg.evaluation.k}: {mAP:.3f}\")\n    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    base  = f\"{name}_{stamp}\"\n\n    if hasattr(extractor, \"state_dict\"):\n        torch.save(extractor.state_dict(), save_dir / f\"{base}.pth\")\n    elif hasattr(extractor, \"backbone\") and hasattr(extractor.backbone, \"state_dict\"):\n        torch.save(extractor.backbone.state_dict(), save_dir / f\"{base}.pth\")\n\n    faiss.write_index(idx, str(save_dir / f\"{base}.faiss\"))\n    np.save(save_dir / f\"{base}_paths.npy\", np.array(rel_paths, dtype=object))\n    print(f\"Saved: {base}.pth / .faiss / _paths.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T13:06:15.130318Z","iopub.execute_input":"2025-07-09T13:06:15.131020Z","iopub.status.idle":"2025-07-09T13:36:28.812818Z","shell.execute_reply.started":"2025-07-09T13:06:15.130996Z","shell.execute_reply":"2025-07-09T13:36:28.812016Z"}},"outputs":[{"name":"stdout","text":"\n=== RESNET50 ===\nFine tune ResNet50\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 01: train loss=0.9206 acc=0.719 | val loss=0.8580 acc=0.697\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02: train loss=0.3313 acc=0.895 | val loss=0.6186 acc=0.766\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03: train loss=0.1885 acc=0.944 | val loss=0.4813 acc=0.835\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04: train loss=0.1232 acc=0.963 | val loss=0.4736 acc=0.812\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 05: train loss=0.0936 acc=0.973 | val loss=0.4357 acc=0.818\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 06: train loss=0.0528 acc=0.990 | val loss=0.4264 acc=0.841\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 07: train loss=0.0438 acc=0.990 | val loss=0.4688 acc=0.838\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 08: train loss=0.0347 acc=0.992 | val loss=0.4347 acc=0.844\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 09: train loss=0.0326 acc=0.991 | val loss=0.4285 acc=0.853\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10: train loss=0.0223 acc=0.996 | val loss=0.4259 acc=0.858\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11: train loss=0.0197 acc=0.997 | val loss=0.4393 acc=0.847\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12: train loss=0.0153 acc=0.999 | val loss=0.4297 acc=0.867\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13: train loss=0.0150 acc=0.997 | val loss=0.4544 acc=0.853\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14: train loss=0.0141 acc=0.998 | val loss=0.4227 acc=0.864\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15: train loss=0.0162 acc=0.997 | val loss=0.4565 acc=0.850\nEarly stopping triggered\nBest val acc=0.867 (epoch 12)\nPrecision@5: 0.926\nmAP@5: 0.826\nSaved: resnet50_20250709_131618.pth / .faiss / _paths.npy\n\n=== EFFICIENTNET_B0 ===\nFine tune EfficientNet-B0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: train loss=0.5469 acc=0.821 | val loss=0.4323 acc=0.870\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02: train loss=0.1488 acc=0.954 | val loss=0.4558 acc=0.858\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03: train loss=0.0574 acc=0.986 | val loss=0.4605 acc=0.867\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04: train loss=0.0280 acc=0.993 | val loss=0.4595 acc=0.882\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 05: train loss=0.0228 acc=0.993 | val loss=0.4137 acc=0.887\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 06: train loss=0.0115 acc=0.997 | val loss=0.4392 acc=0.896\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 07: train loss=0.0103 acc=0.998 | val loss=0.4554 acc=0.893\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 08: train loss=0.0108 acc=0.997 | val loss=0.5013 acc=0.879\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 09: train loss=0.0103 acc=0.997 | val loss=0.4327 acc=0.893\nEarly stopping triggered\nBest val acc=0.896 (epoch 6)\nPrecision@5: 0.938\nmAP@5: 0.895\nSaved: efficientnet_b0_20250709_132110.pth / .faiss / _paths.npy\n\n=== CLIP_ZEROSHOT ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n","output_type":"stream"},{"name":"stdout","text":"Precision@5: 0.548\nmAP@5: 0.279\nSaved: clip_zeroshot_20250709_132153.pth / .faiss / _paths.npy\n\n=== METRIC_LEARNING ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E01 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/functional.py:1483: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return _VF.cdist(x1, x2, p, None)  # type: ignore[attr-defined]\n/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: loss=0.0957 | val kNN@1=0.096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E02 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02: loss=0.0915 | val kNN@1=0.171\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E03 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03: loss=0.0899 | val kNN@1=0.183\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E04 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04: loss=0.0869 | val kNN@1=0.145\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E05 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 05: loss=0.0857 | val kNN@1=0.197\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E06 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 06: loss=0.0856 | val kNN@1=0.188\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E07 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 07: loss=0.0783 | val kNN@1=0.197\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E08 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 08: loss=0.0822 | val kNN@1=0.171\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"E09 train:   0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 09: loss=0.0721 | val kNN@1=0.197\nEarly stopping.\nBest val kNN@1 = 0.197\nPrecision@5: 0.344\nmAP@5: 0.254\nSaved: metric_learning_20250709_132846.pth / .faiss / _paths.npy\n\n=== CLIP_FINETUNE ===\nFine tune CLIP\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01: train loss=1.1290 acc=0.602 | val loss=1.2541 acc=0.338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02: train loss=0.6591 acc=0.750 | val loss=1.4468 acc=0.263\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03: train loss=0.4783 acc=0.833 | val loss=0.7082 acc=0.728\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04: train loss=0.3343 acc=0.885 | val loss=1.2944 acc=0.581\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 05: train loss=0.2209 acc=0.924 | val loss=1.1196 acc=0.688\nEarly stopping triggered\nBest val acc=0.728 (epoch 3)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:217.)\n  return F.linear(input, self.weight, self.bias)\n","output_type":"stream"},{"name":"stdout","text":"Precision@5: 0.331\nmAP@5: 0.238\nSaved: clip_finetune_20250709_133223.pth / .faiss / _paths.npy\n\n=== DINOV2 ===\nPrecision@5: 0.974\nmAP@5: 0.869\nSaved: dinov2_20250709_133628.pth / .faiss / _paths.npy\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# best_name = max(scores, key=scores.get)\n# extr = extractors[best_name]\n# extr.fit(train_loader)\n\n\n# all_paths = [s for _,_,paths in DataLoader(full_ds, batch_size=128) for s in paths]\n# all_feats=[]\n# for x,_,_ in DataLoader(full_ds, batch_size=128):\n#     all_feats.append(extr.encode(x))\n# all_feats = np.vstack(all_feats)\n# index = build_index(all_feats, cfg.index.metric)\n# faiss.write_index(index, f'{best_name}.faiss')\n\n# # веса модели (если DL)\n# torch.save(extr.backbone.state_dict(), f'{best_name}.pth')\n# np.save('paths.npy', np.array(all_paths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T12:07:07.286418Z","iopub.status.idle":"2025-07-09T12:07:07.286713Z","shell.execute_reply.started":"2025-07-09T12:07:07.286550Z","shell.execute_reply":"2025-07-09T12:07:07.286567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/output.zip /kaggle/working/output/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}