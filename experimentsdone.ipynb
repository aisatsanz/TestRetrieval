{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2431805,"sourceType":"datasetVersion","datasetId":8782},{"sourceId":465903,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":375893,"modelId":396661}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        # print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:40:43.229943Z","iopub.execute_input":"2025-07-09T17:40:43.230453Z","iopub.status.idle":"2025-07-09T17:40:47.687402Z","shell.execute_reply.started":"2025-07-09T17:40:43.230430Z","shell.execute_reply":"2025-07-09T17:40:47.686799Z"},"_kg_hide-output":false,"scrolled":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install torchvision torch faiss-cpu opencv-python-headless timm torchmetrics pytorch-metric-learning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:40:47.688435Z","iopub.execute_input":"2025-07-09T17:40:47.688830Z","iopub.status.idle":"2025-07-09T17:41:00.808742Z","shell.execute_reply.started":"2025-07-09T17:40:47.688811Z","shell.execute_reply":"2025-07-09T17:41:00.808004Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.3)\nCollecting pytorch-metric-learning\n  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (4.67.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m177.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C:01\u001b[0m\n\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m177.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport torch\nfrom types import SimpleNamespace\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.use_deterministic_algorithms(True, warn_only=True)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark     = False\nset_seed(42)\n\n\ninput_dirs = glob.glob('/kaggle/input/*flowers*')\nif input_dirs:\n    dataset_root = input_dirs[0]\nelse:\n    dataset_root = '../input/flowers-recognition/flowers'\n\ncfg = SimpleNamespace(\n    seed=42,\n    verbose=True,\n    dataset=SimpleNamespace(\n        root=dataset_root,\n        image_size=224,\n        num_workers=0\n    ),\n    splits=SimpleNamespace(\n        training_ratio=0.8\n    ),\n    training=SimpleNamespace(\n        batch_size=32,\n        epochs=15,\n        lr_backbone=1e-4,\n        lr_head=1e-3\n    ),\n    index=SimpleNamespace(\n        type='hnsw',\n        metric='cosine',\n        hnsw=SimpleNamespace(\n            M=32,\n            ef_construction=200,\n            ef_search=50\n        )\n    ),\n    models=[\n        'resnet50',\n        'efficientnet_b0',\n        'clip_zeroshot',\n        'clip_finetune',\n        'metric_learning',\n        'dinov2',\n        ''\n    ],\n    evaluation=SimpleNamespace(\n        k=5\n    ),\n    api=SimpleNamespace(\n        host='0.0.0.0',\n        port=8000\n    )\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:00.809718Z","iopub.execute_input":"2025-07-09T17:41:00.809953Z","iopub.status.idle":"2025-07-09T17:41:05.158478Z","shell.execute_reply.started":"2025-07-09T17:41:00.809929Z","shell.execute_reply":"2025-07-09T17:41:05.157703Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from pathlib import Path\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport random, os, torch, numpy as np\n\nclass FlowersDataset(Dataset):\n\n    def __init__(self, root, transform=None):\n        self.samples, self.labels = [], []\n        classes = sorted(os.listdir(root))\n        self.class_to_idx = {c:i for i,c in enumerate(classes)}\n        for cls in classes:\n            for p in Path(root, cls).glob('*'):\n                self.samples.append(str(p))\n                self.labels.append(self.class_to_idx[cls])\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.samples[idx]).convert('RGB')\n        if self.transform: img = self.transform(img)\n        return img, self.labels[idx], self.samples[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:05.160343Z","iopub.execute_input":"2025-07-09T17:41:05.160663Z","iopub.status.idle":"2025-07-09T17:41:08.353168Z","shell.execute_reply.started":"2025-07-09T17:41:05.160645Z","shell.execute_reply":"2025-07-09T17:41:08.352468Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"MEAN = (0.48145466, 0.4578275, 0.40821073)\nSTD  = (0.26862954, 0.26130258, 0.27577711)\n\nclip_val_tf = transforms.Compose([\n    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(MEAN, STD),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:08.353773Z","iopub.execute_input":"2025-07-09T17:41:08.354097Z","iopub.status.idle":"2025-07-09T17:41:08.359202Z","shell.execute_reply.started":"2025-07-09T17:41:08.354055Z","shell.execute_reply":"2025-07-09T17:41:08.358184Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def set_seed(seed: int = 42):\n    # 1 Python\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n    # 2 NumPy\n    np.random.seed(seed)\n\n    # 3 PyTorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.use_deterministic_algorithms(True, warn_only=True)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark     = False\n\n    # 4 FAISS (HNSW), sklearn, etc.\n    try:\n        import faiss\n        faiss.rand.seed(seed)\n    except ImportError:\n        pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:08.360223Z","iopub.execute_input":"2025-07-09T17:41:08.360471Z","iopub.status.idle":"2025-07-09T17:41:08.914699Z","shell.execute_reply.started":"2025-07-09T17:41:08.360447Z","shell.execute_reply":"2025-07-09T17:41:08.913974Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"full_ds = FlowersDataset(\"/kaggle/input/flowers-recognition/flowers\", None)\nidxs = list(range(len(full_ds)))\nrandom.Random(42).shuffle(idxs)\n\nfrom collections import defaultdict\ncls_bins = defaultdict(list)\nfor i in idxs:\n    cls_bins[full_ds.labels[i]].append(i)\n\ntrain_idx, test_idx = [], []\nfor lst in cls_bins.values():\n    cut = int(len(lst) * 0.8)\n    train_idx += lst[:cut]\n    test_idx  += lst[cut:]\n\ntrain_set = torch.utils.data.Subset(full_ds, train_idx)\ntest_set  = torch.utils.data.Subset(full_ds, test_idx)\ntrain_set.dataset.transform = clip_val_tf\ntest_set.dataset.transform  = clip_val_tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:08.915437Z","iopub.execute_input":"2025-07-09T17:41:08.915695Z","iopub.status.idle":"2025-07-09T17:41:08.952599Z","shell.execute_reply.started":"2025-07-09T17:41:08.915670Z","shell.execute_reply":"2025-07-09T17:41:08.952048Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import abc\nclass FeatureExtractor(abc.ABC):\n    dim: int\n    @abc.abstractmethod\n    def fit(self, loader): pass\n    @abc.abstractmethod\n    def encode(self, images): pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:08.953298Z","iopub.execute_input":"2025-07-09T17:41:08.953509Z","iopub.status.idle":"2025-07-09T17:41:08.957513Z","shell.execute_reply.started":"2025-07-09T17:41:08.953492Z","shell.execute_reply":"2025-07-09T17:41:08.956905Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from copy import deepcopy\n\nclass FineTuneMixin:\n    def _make_val_split(self, train_subset, val_ratio=1-cfg.splits.training_ratio):\n        idx = train_subset.indices if hasattr(train_subset, \"indices\") else list(range(len(train_subset)))\n        split = int(len(idx) * (1 - val_ratio))\n        return (\n            torch.utils.data.Subset(train_subset.dataset, idx[:split]),\n            torch.utils.data.Subset(train_subset.dataset, idx[split:])\n        )\n\n    def _train_one_epoch(self, loader, criterion, optimizer, scheduler):\n        self.backbone.train()\n        running_loss, correct, total = 0.0, 0, 0\n        for x, y, _ in tqdm(loader, disable=not cfg.verbose, leave=False):\n            x, y = x.cuda(), y.cuda()\n            optimizer.zero_grad()\n            out = self.backbone(x)\n            loss = criterion(out, y)\n            loss.backward()\n            optimizer.step()\n            if scheduler: scheduler.step()\n\n            running_loss += loss.item() * x.size(0)\n            preds = out.argmax(dim=1)\n            correct += (preds == y).sum().item()\n            total += x.size(0)\n        return running_loss / total, correct / total\n\n    @torch.no_grad()\n    def _validate(self, loader, criterion):\n        self.backbone.eval()\n        vl, vc, vt = 0.0, 0, 0\n        for x, y, _ in loader:\n            x, y = x.cuda(), y.cuda()\n            out = self.backbone(x)\n            loss = criterion(out, y)\n            vl += loss.item() * x.size(0)\n            vc += (out.argmax(1) == y).sum().item()\n            vt += x.size(0)\n        return vl / vt, vc / vt\n\n    def _fine_tune(self, train_loader, *, max_epochs=cfg.training.epochs, patience=3,\n                   lr_head=cfg.training.lr_head, lr_base=cfg.training.lr_backbone, weight_decay=1e-4):\n        train_ds, val_ds = self._make_val_split(train_loader.dataset, 0.1)\n        train_dl = torch.utils.data.DataLoader(\n            train_ds, batch_size=train_loader.batch_size, shuffle=True,\n            num_workers=cfg.dataset.num_workers, drop_last=True)\n        val_dl = torch.utils.data.DataLoader(\n            val_ds, batch_size=train_loader.batch_size, shuffle=False,\n            num_workers=cfg.dataset.num_workers)\n\n        optim = torch.optim.AdamW(\n            [\n                {\"params\": self.head_params, \"lr\": lr_head, \"weight_decay\": weight_decay},\n                {\"params\": self.base_params, \"lr\": lr_base, \"weight_decay\": weight_decay}\n            ]\n        )\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=max_epochs*len(train_dl))\n        criterion = getattr(self, \"loss_fn\", nn.CrossEntropyLoss())\n        best_wts, best_acc, wait = deepcopy(self.backbone.state_dict()), 0.0, 0\n\n        for epoch in range(1, max_epochs + 1):\n            tl, ta = self._train_one_epoch(train_dl, criterion, optim, sched)\n            vl, va = self._validate(val_dl, criterion)\n            print(f\"Epoch {epoch:02d}: train loss={tl:.4f} acc={ta:.3f} | \"\n                  f\"val loss={vl:.4f} acc={va:.3f}\")\n            if va > best_acc + 1e-4:\n                best_acc, best_wts, wait = va, deepcopy(self.backbone.state_dict()), 0\n            else:\n                wait += 1\n                if wait >= patience:\n                    print(\"Early stopping triggered\")\n                    break\n\n        self.backbone.load_state_dict(best_wts)\n        print(f\"Best val acc={best_acc:.3f} (epoch {epoch-wait})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:08.958177Z","iopub.execute_input":"2025-07-09T17:41:08.958424Z","iopub.status.idle":"2025-07-09T17:41:08.973655Z","shell.execute_reply.started":"2025-07-09T17:41:08.958399Z","shell.execute_reply":"2025-07-09T17:41:08.973078Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ResNetExtractor(FineTuneMixin, FeatureExtractor):\n    def __init__(self, num_classes=5):\n        self.backbone = timm.create_model('resnet50', pretrained=True, drop_rate=0.2)\n        self.backbone.reset_classifier(num_classes)\n        self.dim = self.backbone.num_features\n\n        classifier = self.backbone.get_classifier()\n        self.head_params = list(classifier.parameters())\n        self.base_params = [p for p in self.backbone.parameters() if id(p) not in set(map(id, self.head_params))]\n\n    def fit(self, train_loader):\n        print(\"Fine tune ResNet50\")\n        self.backbone.cuda()\n        self._fine_tune(train_loader,\n                        max_epochs=cfg.training.epochs,\n                        patience=3,\n                        lr_head=cfg.training.lr_head,\n                        lr_base=cfg.training.lr_backbone)\n\n    @staticmethod\n    def _pool(feats):\n        \"\"\"\n        B*C*H*W  ->  B*C или оставляет если уже\n        \"\"\"\n        if feats.ndim == 4:\n            feats = feats.mean(dim=(-1, -2))\n        elif feats.ndim == 3:\n            feats = feats.squeeze(-1)\n        return torch.nn.functional.normalize(feats, dim=-1)\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            feats = self.backbone.forward_features(images.cuda())\n            #print(feats.shape)\n            feats = self._pool(feats)\n            return feats.cpu().numpy().astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:08.975821Z","iopub.execute_input":"2025-07-09T17:41:08.976006Z","iopub.status.idle":"2025-07-09T17:41:08.993945Z","shell.execute_reply.started":"2025-07-09T17:41:08.975992Z","shell.execute_reply":"2025-07-09T17:41:08.993368Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class EfficientNetExtractor(FineTuneMixin, FeatureExtractor):\n    def __init__(self, num_classes=5):\n        self.backbone = timm.create_model(\n            'efficientnet_b0',\n            pretrained=True,\n            drop_rate=0.2\n        )\n        self.backbone.reset_classifier(num_classes)\n        self.dim = self.backbone.num_features\n        classifier = self.backbone.get_classifier()\n        self.head_params = list(classifier.parameters())\n        self.base_params = [p for p in self.backbone.parameters()\n                            if id(p) not in set(map(id, self.head_params))]\n\n    def fit(self, train_loader):\n        print(\"Fine tune EfficientNet-B0\")\n        self.backbone.cuda()\n        self._fine_tune(train_loader,\n                        max_epochs=cfg.training.epochs,\n                        patience=3,\n                        lr_head=cfg.training.lr_head,\n                        lr_base=cfg.training.lr_backbone)\n    @staticmethod\n    def _pool(feats):\n        \"\"\"\n        B*C*H*W  ->  B*C или оставляет если уже\n        \"\"\"\n        if feats.ndim == 4:\n            feats = feats.mean(dim=(-1, -2))\n        elif feats.ndim == 3:\n            feats = feats.squeeze(-1)\n        return torch.nn.functional.normalize(feats, dim=-1)\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            feats = self.backbone.forward_features(images.cuda())\n            #print(feats.shape)\n            feats = self._pool(feats)\n            return feats.cpu().numpy().astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:08.994703Z","iopub.execute_input":"2025-07-09T17:41:08.995379Z","iopub.status.idle":"2025-07-09T17:41:09.018260Z","shell.execute_reply.started":"2025-07-09T17:41:08.995361Z","shell.execute_reply":"2025-07-09T17:41:09.017618Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip install transformers accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:41:09.019015Z","iopub.execute_input":"2025-07-09T17:41:09.019301Z","iopub.status.idle":"2025-07-09T17:42:29.561596Z","shell.execute_reply.started":"2025-07-09T17:41:09.019284Z","shell.execute_reply":"2025-07-09T17:42:29.560849Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\nUsing cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\nDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nclass CLIPHFExtractor(FeatureExtractor):\n    \"\"\"\n    CLIP ViT-B/32 по умолчанию без fine tune\n    \"\"\"\n    def __init__(self,\n                 model_name: str = \"openai/clip-vit-base-patch32\",\n                 device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        self.device = device\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.model     = CLIPModel.from_pretrained(model_name).to(device)\n        self.model.eval()\n        self.dim = self.model.config.projection_dim\n        self.to_pil = transforms.ToPILImage()\n\n    def fit(self, *_): pass\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            pil = [self.to_pil(img.cpu()) for img in images]\n        else:\n            pil = [Image.open(p).convert(\"RGB\") for p in images]\n        inputs = self.processor(images=pil, return_tensors=\"pt\", padding=True).to(self.device)\n        feats = self.model.get_image_features(**inputs)\n        feats = F.normalize(feats, p=2, dim=-1)\n        return feats.cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:42:29.562579Z","iopub.execute_input":"2025-07-09T17:42:29.562810Z","iopub.status.idle":"2025-07-09T17:42:47.461383Z","shell.execute_reply.started":"2025-07-09T17:42:29.562788Z","shell.execute_reply":"2025-07-09T17:42:47.460751Z"}},"outputs":[{"name":"stderr","text":"2025-07-09 17:42:33.051743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752082953.212122      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752082953.257056      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install pytorch_metric_learning","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:26.236572Z","iopub.execute_input":"2025-07-09T17:49:26.237331Z","iopub.status.idle":"2025-07-09T17:49:29.725267Z","shell.execute_reply.started":"2025-07-09T17:49:26.237307Z","shell.execute_reply":"2025-07-09T17:49:29.724495Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorch_metric_learning\n  Using cached pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch_metric_learning) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pytorch_metric_learning) (1.2.2)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_metric_learning) (2.6.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch_metric_learning) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->pytorch_metric_learning) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch_metric_learning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch_metric_learning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch_metric_learning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch_metric_learning) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch_metric_learning) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pytorch_metric_learning) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch_metric_learning) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch_metric_learning) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch_metric_learning) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->pytorch_metric_learning) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pytorch_metric_learning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pytorch_metric_learning) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pytorch_metric_learning) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pytorch_metric_learning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pytorch_metric_learning) (2024.2.0)\nDownloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pytorch_metric_learning\nSuccessfully installed pytorch_metric_learning-2.8.1\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from pytorch_metric_learning import losses, miners, samplers\nimport torch.nn as nn\n\n\nclass MetricExtractor(nn.Module):\n    \"\"\"\n    ResNet-18 backbone + Linear(256) + TripletLoss\n    \"\"\"\n    def __init__(self, embed_dim: int = 256):\n        super().__init__()\n        self.backbone = timm.create_model(\n            \"resnet18\", pretrained=True, num_classes=0, drop_rate=0.2\n        )\n        self.embed = nn.Linear(self.backbone.num_features, embed_dim)\n        self.dim   = embed_dim\n\n        self.loss_fn = losses.TripletMarginLoss(margin=0.2)\n        self.miner   = miners.TripletMarginMiner(\n            margin=0.2, type_of_triplets=\"semihard\"\n        )\n\n    def fit(\n        self,\n        dl: DataLoader,\n        *,\n        epochs: int = cfg.training.epochs,\n        patience: int = 4,\n        lr_head: float = cfg.training.lr_head,\n        lr_base: float = cfg.training.lr_backbone,\n        weight_decay: float = 1e-4,\n    ):\n        idx = dl.dataset.indices if hasattr(dl.dataset, \"indices\") else range(len(dl.dataset))\n        n_val = int(0.1 * len(idx))\n        train_idx, val_idx = idx[:-n_val], idx[-n_val:]\n\n        base_ds = dl.dataset.dataset if hasattr(dl.dataset, \"dataset\") else dl.dataset\n        train_ds = Subset(base_ds, train_idx)\n        val_ds   = Subset(base_ds, val_idx)\n\n        train_dl = DataLoader(\n            train_ds, batch_size=dl.batch_size, shuffle=True,\n            num_workers=cfg.dataset.num_workers, drop_last=True\n        )\n        val_dl = DataLoader(\n            val_ds, batch_size=dl.batch_size, shuffle=False,\n            num_workers=cfg.dataset.num_workers\n        )\n\n        opt = torch.optim.AdamW(\n            [\n                {\"params\": self.embed.parameters(), \"lr\": lr_head, \"weight_decay\": weight_decay},\n                {\"params\": self.backbone.parameters(), \"lr\": lr_base, \"weight_decay\": weight_decay},\n            ]\n        )\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt, T_max=epochs * len(train_dl)\n        )\n\n        self.backbone.cuda(); self.embed.cuda()\n\n        best_knn, wait = 0.0, 0\n        best_state     = {k: v.clone() for k, v in self.state_dict().items()}\n\n        for ep in range(1, epochs + 1):\n\n            self.train()\n            run_loss, seen = 0.0, 0\n            for x, y, _ in tqdm(train_dl, leave=False, desc=f\"E{ep:02d} train\"):\n                x, y = x.cuda(), y.cuda()\n                opt.zero_grad()\n                emb = F.normalize(self.embed(self.backbone(x)), dim=-1)\n                hard = self.miner(emb, y)\n                loss = self.loss_fn(emb, y, hard)\n                loss.backward(); opt.step(); sched.step()\n                run_loss += loss.item() * x.size(0)\n                seen     += x.size(0)\n            train_loss = run_loss / seen\n            knn_acc = self._val_knn_acc(train_dl, val_dl, k=5)\n\n            print(f\"Epoch {ep:02d}: loss={train_loss:.4f} | val kNN@1={knn_acc:.3f}\")\n\n            if knn_acc > best_knn + 1e-4:\n                best_knn, best_state, wait = knn_acc, \\\n                    {k: v.clone() for k, v in self.state_dict().items()}, 0\n            else:\n                wait += 1\n                if wait >= patience:\n                    print(\"Early stopping.\")\n                    break\n\n        self.load_state_dict(best_state)\n        print(f\"Best val kNN@1 = {best_knn:.3f}\")\n\n    @torch.no_grad()\n    def _val_knn_acc(self, train_dl: DataLoader, val_dl: DataLoader, *, k: int = 5) -> float:\n        \"\"\"\n        Строит Flat-индекс из train-эмбеддингов и меряет top-1 точность на val\n        \"\"\"\n        tr_vecs, tr_lbls = [], []\n        for x, y, _ in train_dl:\n            z = F.normalize(self.embed(self.backbone(x.cuda())), dim=-1)\n            tr_vecs.append(z.cpu()); tr_lbls.append(y)\n        tr_vecs = torch.cat(tr_vecs).numpy().astype(\"float32\")\n        tr_lbls = torch.cat(tr_lbls).numpy()\n\n        faiss.normalize_L2(tr_vecs)\n        index = faiss.IndexFlatIP(tr_vecs.shape[1])\n        index.add(tr_vecs)\n\n        correct, total = 0, 0\n        for x, y, _ in val_dl:\n            q = F.normalize(self.embed(self.backbone(x.cuda())), dim=-1).cpu().numpy()\n            faiss.normalize_L2(q)\n            _, I = index.search(q, k)                # (B,k)\n            pred = tr_lbls[I[:, 0]]                  # ближайший сосед\n            correct += np.sum(pred == y.numpy())\n            total   += y.size(0)\n        return correct / total\n\n    @torch.no_grad()\n    def encode(self, images):\n        \"\"\"Возвращает numpy (B,D)\"\"\"\n        if isinstance(images, torch.Tensor):\n            x = images.cuda()\n        else:\n            x = torch.stack([clip_val_tf(Image.open(p).convert(\"RGB\")) for p in images]).cuda()\n\n        z = F.normalize(self.embed(self.backbone(x)), dim=-1)\n        return z.cpu().numpy().astype(\"float32\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:30.740872Z","iopub.execute_input":"2025-07-09T17:49:30.741218Z","iopub.status.idle":"2025-07-09T17:49:30.787680Z","shell.execute_reply.started":"2025-07-09T17:49:30.741190Z","shell.execute_reply":"2025-07-09T17:49:30.787120Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class FastMetricExtractor(nn.Module):\n    def __init__(self, embed_dim=512, n_classes=5):\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.backbone = timm.create_model(\"resnet50\", pretrained=True, num_classes=0)\n        for name, p in self.backbone.named_parameters():\n            if not name.startswith(\"layer4\"):       \n                p.requires_grad_(False)\n\n        self.embed = nn.Linear(self.backbone.num_features, embed_dim)\n        self.loss  = losses.ProxyAnchorLoss(n_classes, embed_dim).to(self.device)\n\n    def forward(self, x):\n        return self.backbone(x)\n\n\n    def fit(self, dl: DataLoader, *, epochs=5, lr_backbone=1e-5, lr_head=1e-4):\n\n        sampler = samplers.MPerClassSampler(dl.dataset, m=2,\n                                            length_before_new_iter=len(dl.dataset))\n        train_dl = DataLoader(dl.dataset,\n                              batch_size = cfg.training.batch_size,\n                              sampler    = sampler,\n                              num_workers= cfg.dataset.num_workers,\n                              drop_last  = True)\n\n\n        opt = torch.optim.AdamW([\n            {\"params\": self.embed.parameters(),                 \"lr\": lr_head},\n            {\"params\": filter(lambda p: p.requires_grad,\n                              self.backbone.parameters()),      \"lr\": lr_backbone},\n            {\"params\": self.loss.parameters(),                  \"lr\": lr_head}\n        ])\n\n        self.to(self.device)\n        best_map, best_state = 0., None\n\n        for ep in range(1, epochs + 1):\n            self.train()\n            loop = tqdm(train_dl, total=len(train_dl),\n                        desc=f\"E{ep:02d} train\", leave=False)\n\n            for x, y, _ in loop:\n                x, y = x.to(self.device), y.to(self.device)\n\n                opt.zero_grad()\n                emb  = F.normalize(self.embed(self.backbone(x)), dim=-1)\n                loss = self.loss(emb, y)\n                loss.backward()\n                opt.step()\n\n                loop.set_postfix(loss=f\"{loss.item():.4f}\")\n\n    \n            p5_val, map_val = self._quick_map(train_dl, k=5)\n            print(f\"E{ep:02d}: P@5={p5_val:.3f} | mAP@5={map_val:.3f}\")\n\n            if map_val > best_map:\n                best_map, best_state = map_val, {k: v.clone()\n                                                 for k, v in self.state_dict().items()}\n\n        self.load_state_dict(best_state)\n        print(f\"✔ Best mAP={best_map:.3f}\")\n\n\n    @torch.no_grad()\n    def _quick_map(self, dl, k: int = 5):\n        vecs, lbls = [], []\n        for x, y, _ in tqdm(dl, desc=\"encode val\", leave=False):\n            z = F.normalize(self.embed(self.backbone(x.to(self.device))), dim=-1).cpu()\n            vecs.append(z); lbls.append(y)\n        vecs = torch.cat(vecs).numpy().astype(\"float32\")\n        lbls = torch.cat(lbls).numpy()\n\n        faiss.normalize_L2(vecs)\n        index = faiss.IndexFlatIP(vecs.shape[1])\n        if faiss.get_num_gpus():\n            index = faiss.index_cpu_to_all_gpus(index)\n        index.add(vecs)\n\n        D, I = index.search(vecs, k)\n        rel  = (lbls[I] == lbls[:, None]).astype(int)\n\n        p_at_k = rel.sum(axis=1) / k\n        mean_p = float(p_at_k.mean())\n\n        precisions = rel.cumsum(1) / np.arange(1, k + 1)\n        map_k = float((precisions * rel).sum() / rel.sum())\n\n        return mean_p, map_k\n\n    @torch.no_grad()\n    def encode(self, x: torch.Tensor) -> np.ndarray:\n        self.eval()\n        x   = x.to(self.device, non_blocking=True)\n        emb = F.normalize(self.embed(self.backbone(x)), dim=-1)\n        return emb.cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:33.232188Z","iopub.execute_input":"2025-07-09T17:49:33.232483Z","iopub.status.idle":"2025-07-09T17:49:33.245787Z","shell.execute_reply.started":"2025-07-09T17:49:33.232462Z","shell.execute_reply":"2025-07-09T17:49:33.245194Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class _ClipImageEncoder(nn.Module):\n    def __init__(self, clip_model):\n        super().__init__()\n        self.clip = clip_model\n\n    def forward(self, x):                 # x: B*3*H*W\n        return self.clip.get_image_features(pixel_values=x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:35.696948Z","iopub.execute_input":"2025-07-09T17:49:35.697524Z","iopub.status.idle":"2025-07-09T17:49:35.701880Z","shell.execute_reply.started":"2025-07-09T17:49:35.697501Z","shell.execute_reply":"2025-07-09T17:49:35.701147Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class _Wrapper:\n    def __init__(self, loader, fn_map):\n        self.loader = loader\n        self.fn_map = fn_map\n        self.batch_size = loader.batch_size\n        self.dataset = loader.dataset\n    def __iter__(self):\n        return self.fn_map(self.loader)\n    def __len__(self):\n        return len(self.loader)\n\nclass CLIPFineTuneExtractor(FineTuneMixin, FeatureExtractor):\n    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.clip      = CLIPModel.from_pretrained(model_name).to(self.device)\n        self.dim       = self.clip.config.projection_dim\n        self.head_params = list(self.clip.visual_projection.parameters())\n        self.base_params = list(self.clip.vision_model.parameters())\n\n        for p in self.clip.text_model.parameters():\n            p.requires_grad = False\n\n        self.backbone = _ClipImageEncoder(self.clip)\n        self.t2pil    = transforms.ToPILImage()\n\n    def fit(self, train_loader):\n        print(\"Fine tune CLIP\")\n\n        def _tensor_loader(dl):\n            for x, y, pths in dl:\n                pil = [self.t2pil(img.cpu()) for img in x]\n                px  = self.processor(images=pil, return_tensors=\"pt\", padding=True)[\"pixel_values\"]\n                yield px, y, pths\n\n        wrapped = _Wrapper(train_loader, _tensor_loader)\n        self.clip.train()\n        self._fine_tune(\n            wrapped,\n            max_epochs=cfg.training.epochs,\n            patience=2,\n            lr_head=cfg.training.lr_head,\n            lr_base=cfg.training.lr_backbone\n        )\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            pil = [self.t2pil(img.cpu()) for img in images]\n        else:\n            pil = [Image.open(p).convert(\"RGB\") for p in images]\n\n        px = self.processor(images=pil, return_tensors=\"pt\", padding=True)[\"pixel_values\"].to(self.device)\n        feats = self.clip.get_image_features(pixel_values=px)\n        return F.normalize(feats, p=2, dim=-1).cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:35.916941Z","iopub.execute_input":"2025-07-09T17:49:35.917650Z","iopub.status.idle":"2025-07-09T17:49:35.927282Z","shell.execute_reply.started":"2025-07-09T17:49:35.917624Z","shell.execute_reply":"2025-07-09T17:49:35.926588Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class DINOv2Extractor(FeatureExtractor):\n    def __init__(self, model_name: str = \"vit_base_patch14_dinov2\"):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.IMG = 518\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=True,\n            num_classes=0,\n            img_size=self.IMG,          # задаём правильный размер тк дино на 518\n        ).to(self.device).eval()\n\n        self.dim = self.backbone.num_features\n\n        self.tf = transforms.Compose([\n            transforms.Resize(self.IMG, interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.CenterCrop(self.IMG),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n                                 std=(0.229, 0.224, 0.225)),\n        ])\n\n    def fit(self, *_): \n        pass\n\n    @torch.no_grad()\n    def encode(self, images):\n        if isinstance(images, torch.Tensor):\n            x = images.to(self.device)\n            if x.shape[-1] != self.IMG:\n                x = F.interpolate(x, size=self.IMG, mode=\"bicubic\", align_corners=False)\n        else:\n            x = torch.stack([self.tf(Image.open(p).convert(\"RGB\")) for p in images]).to(self.device)\n\n        feats = self.backbone(x)            # B * 768\n        feats = F.normalize(feats, dim=-1)\n        return feats.cpu().numpy().astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:37.488500Z","iopub.execute_input":"2025-07-09T17:49:37.488813Z","iopub.status.idle":"2025-07-09T17:49:37.496299Z","shell.execute_reply.started":"2025-07-09T17:49:37.488789Z","shell.execute_reply":"2025-07-09T17:49:37.495491Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:46.084837Z","iopub.execute_input":"2025-07-09T17:49:46.085605Z","iopub.status.idle":"2025-07-09T17:49:50.728848Z","shell.execute_reply.started":"2025-07-09T17:49:46.085581Z","shell.execute_reply":"2025-07-09T17:49:50.727901Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Using cached faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.11.0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import faiss\n\n\ndef build_index(vecs: np.ndarray, metric: str = \"cosine\") -> faiss.Index:\n    \"\"\"\n    vecs – (N,D) float32,\n    metric: 'cosine' | 'l2'\n    \"\"\"\n    vecs = np.ascontiguousarray(vecs, dtype=\"float32\")\n\n    if metric == \"cosine\":\n        faiss.normalize_L2(vecs)\n        index = faiss.IndexFlatIP(vecs.shape[1])\n    elif metric == \"l2\":\n        index = faiss.IndexFlatL2(vecs.shape[1])\n    else:\n        raise ValueError(\"метрика должна быть 'cosine' or 'l2'\")\n\n    index.add(vecs)\n    return index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:50.730475Z","iopub.execute_input":"2025-07-09T17:49:50.730720Z","iopub.status.idle":"2025-07-09T17:49:50.772206Z","shell.execute_reply.started":"2025-07-09T17:49:50.730697Z","shell.execute_reply":"2025-07-09T17:49:50.771442Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom sklearn.metrics import average_precision_score\n\ndef evaluate(extractor, train_loader, test_loader):\n    \"\"\"\n    → Precision@5, mAP, faiss-index, rel_paths\n    \"\"\"\n    extractor.fit(train_loader)\n\n    enc_loader = DataLoader(\n        train_loader.dataset,\n        batch_size=train_loader.batch_size,\n        shuffle=False,\n        num_workers=cfg.dataset.num_workers,\n    )\n\n    db_vecs, db_labels, rel_paths = [], [], []\n    for x, y, paths in enc_loader:\n        db_vecs.append(extractor.encode(x))\n        db_labels.extend(y.numpy())\n        rel_paths.extend([os.path.relpath(p, cfg.dataset.root) for p in paths])\n\n    db_vecs   = np.vstack(db_vecs).astype(\"float32\")\n    db_labels = np.asarray(db_labels)\n\n    if cfg.index.metric == \"cosine\":\n        faiss.normalize_L2(db_vecs)\n        index = faiss.IndexFlatIP(db_vecs.shape[1])\n    else:\n        index = faiss.IndexFlatL2(db_vecs.shape[1])\n    index.add(db_vecs)\n\n    k = cfg.evaluation.k\n    prec_sum, ap_list = 0.0, []\n    for x, y, _ in test_loader:\n        q = extractor.encode(x)\n        if cfg.index.metric == \"cosine\":\n            faiss.normalize_L2(q)\n        D, I = index.search(q, db_vecs.shape[0])  \n        for lbl, d_row, i_row in zip(y.numpy(), D, I):\n            prec_sum += np.sum(db_labels[i_row[:k]] == lbl) / k\n            rel = (db_labels[i_row] == lbl).astype(int)\n            score = d_row if cfg.index.metric != \"l2\" else -d_row\n            ap_list.append(average_precision_score(rel, score))\n\n    precision = prec_sum / len(test_loader.dataset)\n    mAP = float(np.mean(ap_list))\n    return precision, mAP, index, rel_paths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:50.772888Z","iopub.execute_input":"2025-07-09T17:49:50.773097Z","iopub.status.idle":"2025-07-09T17:49:50.781237Z","shell.execute_reply.started":"2025-07-09T17:49:50.773075Z","shell.execute_reply":"2025-07-09T17:49:50.780646Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from collections import Counter\nfrom pathlib import Path\nfrom datetime import datetime\nimport torch.nn.functional as F\nfrom torch.utils.data import Subset\nimport math\n\nimport timm\n\nextractors = {\n    'resnet50'      : ResNetExtractor(),\n    'efficientnet_b0': EfficientNetExtractor(),\n    'clip_zeroshot' : CLIPHFExtractor(),\n    'clip_finetune' : CLIPFineTuneExtractor(),\n    'dinov2'        : DINOv2Extractor(),\n    'metric_learning' : FastMetricExtractor()\n}\n\ntrain_loader = DataLoader(train_set, batch_size=cfg.training.batch_size, \n                          shuffle=False, num_workers=cfg.dataset.num_workers, drop_last=True)\ntest_loader  = DataLoader(test_set,  batch_size=cfg.training.batch_size, \n                          shuffle=False, num_workers=cfg.dataset.num_workers)\nsave_dir = Path(\"checkpoints\")\nsave_dir.mkdir(exist_ok=True, parents=True)\nscores = {}\nfor name, extractor in extractors.items():\n    print(f\"\\n=== {name.upper()} ===\")\n    p, mAP, idx, rel_paths = evaluate(extractor, train_loader, test_loader)\n    scores[name] = (p, mAP)\n    print(f\"Precision@{cfg.evaluation.k}: {p:.3f}\")\n    print(f\"mAP@{cfg.evaluation.k}: {mAP:.3f}\")\n    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    base  = f\"{name}_{stamp}\"\n\n    if hasattr(extractor, \"state_dict\"):\n        torch.save(extractor.state_dict(), save_dir / f\"{base}.pth\")\n    elif hasattr(extractor, \"backbone\") and hasattr(extractor.backbone, \"state_dict\"):\n        torch.save(extractor.backbone.state_dict(), save_dir / f\"{base}.pth\")\n\n    faiss.write_index(idx, str(save_dir / f\"{base}.faiss\"))\n    np.save(save_dir / f\"{base}_paths.npy\", np.array(rel_paths, dtype=object))\n    print(f\"Saved: {base}.pth / .faiss / _paths.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:49:52.253804Z","iopub.execute_input":"2025-07-09T17:49:52.254387Z","execution_failed":"2025-07-09T17:50:14.128Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbaf47d2ca084fb898d763ebe4f7f1c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c267eb8d1b99489d8b5022c9720fb697"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f2794c211f489785ec2cd1e76cce97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cddf38a659a4033b7009a23e87afb7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9bb0b837ebf4db185da2af66beae122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd7acd70c7c4ce6b78047c8db1094c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f75cf73e834e48823376508c2b3424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c0dd956848404b95ed8f36f192fdc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c5555fb3e34936ac18b8490e661852"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9450b0552c542ff85a0ccb8e154548d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc59f1e4a8ba47a8830fcff9b8484d24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f03bca12564a438ceba3b243291815"}},"metadata":{}},{"name":"stdout","text":"\n=== RESNET50 ===\nFine tune ResNet50\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/97 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91732618fba24cebaf5579e3efc1f5be"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torchvision.transforms as T\nimport torchvision\nimport matplotlib.pyplot as plt\ndevice = \"cuda\"\nROOT = Path('/kaggle/input/resnet50/pytorch/default/1')\nWEIGHTS_PATH = ROOT / 'resnet50_20250709_131618.pth'     \nPATHS_NPY    = ROOT / 'resnet50_20250709_131618_paths.npy' \nFAISS_INDEX  = ROOT / 'resnet50_20250709_131618.faiss'     \nDATASET_ROOT = Path('/kaggle/input/flowers-recognition')\n_rn50 = torchvision.models.resnet50(weights=None)\n\nstate_dict = torch.load(WEIGHTS_PATH, map_location='cpu')\n\n\nfor key in ['fc.weight', 'fc.bias']:\n    state_dict.pop(key, None)\n\n_rn50.load_state_dict(state_dict, strict=False)\n\nfeature_extractor = torch.nn.Sequential(*list(_rn50.children())[:-1]).to(device).eval()\n\n_preprocess = T.Compose([\n    T.Resize(256),\n    T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std =[0.229, 0.224, 0.225]),\n])\n\ndef _embed_image(img_path: Path) -> torch.Tensor:\n    img = Image.open(img_path).convert('RGB')\n    t = _preprocess(img).unsqueeze(0).to(device)\n    with torch.no_grad():\n        emb = feature_extractor(t).squeeze().cpu()\n    return emb\n\n# --- 4. Индекс + пути ---\nraw_paths = np.load(PATHS_NPY, allow_pickle=True) \ndataset_paths = np.array([\n    (DATASET_ROOT / p).resolve() if not Path(p).is_absolute() else Path(p)\n    for p in raw_paths\n], dtype=str)\n\nindex = faiss.read_index(str(FAISS_INDEX))\n\ndef _normalize(v: np.ndarray):\n    v /= (np.linalg.norm(v) + 1e-12)\n    return v\n\ndef find_similar_faiss(query_path: str | Path, topk: int = 5):\n    \"\"\"Возвращает (path, similarity) из уже построенного FAISS-индекса.\"\"\"\n    emb = _embed_image(Path(query_path)).numpy().astype('float32')\n    emb = _normalize(emb)                                  \n    D, I = index.search(emb[None, :], topk)               \n    sims = 1 - D[0] if index.metric_type == faiss.METRIC_L2 else D[0]\n    return [(str(dataset_paths[i]), float(sims[j])) for j, i in enumerate(I[0])]\n\ndef show_results(query_path: str | Path, topk: int = 5):\n    res = find_similar_faiss(query_path, topk)\n    fig, axes = plt.subplots(1, topk + 1, figsize=(3*(topk+1), 3))\n    axes[0].imshow(Image.open(query_path).convert('RGB'))\n    axes[0].set_title('Query')\n    axes[0].axis('off')\n    for idx, (p, s) in enumerate(res, 1):\n        axes[idx].imshow(Image.open(p).convert('RGB'))\n        axes[idx].set_title(f'{s:.3f}')\n        axes[idx].axis('off')\n    plt.tight_layout(); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:42:47.524318Z","iopub.status.idle":"2025-07-09T17:42:47.524611Z","shell.execute_reply.started":"2025-07-09T17:42:47.524464Z","shell.execute_reply":"2025-07-09T17:42:47.524477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_results(\"/kaggle/input/flowers-recognition/flowers/rose/10090824183_d02c613f10_m.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:42:47.525713Z","iopub.status.idle":"2025-07-09T17:42:47.526033Z","shell.execute_reply.started":"2025-07-09T17:42:47.525861Z","shell.execute_reply":"2025-07-09T17:42:47.525876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# best_name = max(scores, key=scores.get)\n# extr = extractors[best_name]\n# extr.fit(train_loader)\n\n\n# all_paths = [s for _,_,paths in DataLoader(full_ds, batch_size=128) for s in paths]\n# all_feats=[]\n# for x,_,_ in DataLoader(full_ds, batch_size=128):\n#     all_feats.append(extr.encode(x))\n# all_feats = np.vstack(all_feats)\n# index = build_index(all_feats, cfg.index.metric)\n# faiss.write_index(index, f'{best_name}.faiss')\n\n# # веса модели (если DL)\n# torch.save(extr.backbone.state_dict(), f'{best_name}.pth')\n# np.save('paths.npy', np.array(all_paths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:42:47.527369Z","iopub.status.idle":"2025-07-09T17:42:47.527676Z","shell.execute_reply.started":"2025-07-09T17:42:47.527538Z","shell.execute_reply":"2025-07-09T17:42:47.527552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}